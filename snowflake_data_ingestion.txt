CREATE DATABASE DATA_PIPELINING;

USE DATABASE DATA_PIPELINING;

CREATE TABLE PATIENTS (
PATIENT_ID INT,
FIRST_NAME VARCHAR(100),
CITY VARCHAR(100),
REGISTRATION_YEAR INT);

--create storage integration
CREATE OR REPLACE STORAGE integration ss_s3_int
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = s3
ENABLED = TRUE
STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::069010008105:role/snowpipie-data-ingestion'
STORAGE_ALLOWED_LOCATIONS = ('s3://patientdb/');

DESC integration ss_s3_int;

-- create the stage
CREATE OR REPLACE STAGE PATIENTS_SHOW_STAGE
URL = 's3://patientdb'
--credentials = (aws_key_id = 'AKIARAEKRFAUULBFCGFX' aws_secret_key = 'R7sCjaf2Ra5vBSXoV1PYqjiDRBqqaupz0SBgaSdl')
file_format = CSV
STORAGE_INTEGRATION = ss_s3_int;

LIST @PATIENTS_SHOW_STAGE;

SHOW STAGES;

-- CREATE SNOWPIPE THAT ARE INGESTED FROM EXTERNAL STAGE AND COPIES THE DATA INTO PATIENTS TABLE
-- THE AUTO_INGEST = TRUE PARAMETER SPECIFIES TO READ EVENT MOTIFICATIONS SENT FROM AN S3 BUCKET TO AN SQS QUEUE WHEN NEW DATA IS READY TO LOAD.

CREATE OR REPLACE PIPE PATIENTS_SNOWPIPE AUTO_INGEST = TRUE AS
COPY INTO "DATA_PIPELINING"."PUBLIC"."PATIENTS"
FROM @PATIENTS_SHOW_STAGE
FILE_FORMAT = (type = 'csv');

SHOW PIPES; 

SELECT * FROM PATIENTS;